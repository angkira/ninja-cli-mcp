{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü•∑ Model Comparison: Cost & Performance Analysis\n",
    "\n",
    "This notebook compares different AI models on simple coding tasks:\n",
    "- **Qwen 2.5 Coder 32B** - Specialized coding model\n",
    "- **Claude Sonnet 4** - Latest Claude model\n",
    "- **Claude Haiku 4** - Fast and cost-effective\n",
    "- **Gemini 2.0 Flash** - Google's latest model\n",
    "\n",
    "We'll measure:\n",
    "- üí∞ Cost per task\n",
    "- ‚ö° Speed (tokens/sec)\n",
    "- üìä Token usage\n",
    "- ‚úÖ Success rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Setup plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, str(Path.cwd().parent / \"src\"))\n",
    "\n",
    "from ninja_cli_mcp.metrics import MetricsTracker\n",
    "from ninja_cli_mcp.qwen_driver import NinjaDriver, NinjaConfig\n",
    "\n",
    "print(\"‚úÖ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Test Configuration\n",
    "\n",
    "Define the models to compare and simple test tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models to compare\n",
    "MODELS = {\n",
    "    \"qwen-2.5-coder-32b\": \"qwen/qwen-2.5-coder-32b-instruct\",\n",
    "    \"claude-sonnet-4\": \"anthropic/claude-sonnet-4\",\n",
    "    \"claude-haiku-4\": \"anthropic/claude-haiku-4\",\n",
    "    \"gemini-2.0-flash\": \"google/gemini-2.0-flash-exp:free\",\n",
    "}\n",
    "\n",
    "# Simple test tasks (designed to use minimal tokens)\n",
    "TEST_TASKS = [\n",
    "    {\n",
    "        \"name\": \"Add Docstring\",\n",
    "        \"description\": \"Add a docstring to the greet function\",\n",
    "        \"code\": '''def greet(name):\n",
    "    return f\"Hello, {name}!\"''',\n",
    "        \"expected_tokens\": 100,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Fix Bug\",\n",
    "        \"description\": \"Fix the off-by-one error in range\",\n",
    "        \"code\": '''def count_to_ten():\n",
    "    for i in range(1, 10):  # BUG: should be 11\n",
    "        print(i)''',\n",
    "        \"expected_tokens\": 150,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Add Type Hints\",\n",
    "        \"description\": \"Add type hints to the function\",\n",
    "        \"code\": '''def add(a, b):\n",
    "    return a + b''',\n",
    "        \"expected_tokens\": 80,\n",
    "    },\n",
    "]\n",
    "\n",
    "print(f\"üéØ Testing {len(MODELS)} models on {len(TEST_TASKS)} tasks\")\n",
    "print(f\"üìä Total tests: {len(MODELS) * len(TEST_TASKS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Run Experiments\n",
    "\n",
    "Execute each task with each model and collect metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import subprocess\n",
    "\n",
    "results = []\n",
    "\n",
    "# Create temp directory for test files\n",
    "test_dir = Path(tempfile.mkdtemp(prefix=\"ninja_test_\"))\n",
    "print(f\"üìÅ Test directory: {test_dir}\")\n",
    "\n",
    "try:\n",
    "    for model_name, model_id in MODELS.items():\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"ü§ñ Testing model: {model_name}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        for task in TEST_TASKS:\n",
    "            print(f\"\\n  üìù Task: {task['name']}\")\n",
    "            \n",
    "            # Create test file\n",
    "            test_file = test_dir / f\"test_{task['name'].lower().replace(' ', '_')}.py\"\n",
    "            test_file.write_text(task['code'])\n",
    "            \n",
    "            # Run task with model\n",
    "            start_time = time.time()\n",
    "            \n",
    "            result = subprocess.run(\n",
    "                [\n",
    "                    \"uv\", \"run\", \"python\", \"-m\", \"ninja_cli_mcp.cli\",\n",
    "                    \"quick-task\",\n",
    "                    \"--repo-root\", str(test_dir),\n",
    "                    \"--task\", task['description'],\n",
    "                    \"--model\", model_id,\n",
    "                    \"--allowed-globs\", \"*.py\",\n",
    "                ],\n",
    "                capture_output=True,\n",
    "                text=True,\n",
    "                timeout=30,\n",
    "                cwd=Path.cwd().parent,\n",
    "            )\n",
    "            \n",
    "            duration = time.time() - start_time\n",
    "            \n",
    "            # Parse output for metrics\n",
    "            output = result.stdout + result.stderr\n",
    "            \n",
    "            # Extract metrics (simplified - you can enhance this)\n",
    "            import re\n",
    "            \n",
    "            input_tokens = 0\n",
    "            output_tokens = 0\n",
    "            cost = 0.0\n",
    "            \n",
    "            # Look for token counts\n",
    "            input_match = re.search(r'Input tokens?:\\s*(\\d+)', output, re.IGNORECASE)\n",
    "            output_match = re.search(r'Output tokens?:\\s*(\\d+)', output, re.IGNORECASE)\n",
    "            cost_match = re.search(r'Cost:\\s*\\$(\\d+\\.\\d+)', output, re.IGNORECASE)\n",
    "            \n",
    "            if input_match:\n",
    "                input_tokens = int(input_match.group(1))\n",
    "            if output_match:\n",
    "                output_tokens = int(output_match.group(1))\n",
    "            if cost_match:\n",
    "                cost = float(cost_match.group(1))\n",
    "            \n",
    "            total_tokens = input_tokens + output_tokens\n",
    "            tokens_per_sec = total_tokens / duration if duration > 0 else 0\n",
    "            \n",
    "            result_data = {\n",
    "                'model': model_name,\n",
    "                'model_id': model_id,\n",
    "                'task': task['name'],\n",
    "                'success': result.returncode == 0,\n",
    "                'duration': duration,\n",
    "                'input_tokens': input_tokens,\n",
    "                'output_tokens': output_tokens,\n",
    "                'total_tokens': total_tokens,\n",
    "                'tokens_per_sec': tokens_per_sec,\n",
    "                'cost': cost,\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "            }\n",
    "            \n",
    "            results.append(result_data)\n",
    "            \n",
    "            # Print summary\n",
    "            status = \"‚úÖ\" if result_data['success'] else \"‚ùå\"\n",
    "            print(f\"    {status} Duration: {duration:.2f}s\")\n",
    "            print(f\"    üìä Tokens: {total_tokens} (in: {input_tokens}, out: {output_tokens})\")\n",
    "            print(f\"    üí∞ Cost: ${cost:.6f}\")\n",
    "            print(f\"    ‚ö° Speed: {tokens_per_sec:.1f} tok/s\")\n",
    "            \n",
    "            # Small delay to avoid rate limits\n",
    "            time.sleep(1)\n",
    "            \n",
    "finally:\n",
    "    # Cleanup\n",
    "    import shutil\n",
    "    shutil.rmtree(test_dir, ignore_errors=True)\n",
    "    print(f\"\\nüßπ Cleaned up test directory\")\n",
    "\n",
    "print(f\"\\n‚úÖ Completed {len(results)} tests\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Display results table\n",
    "print(\"\\nüìã Complete Results:\")\n",
    "display_cols = ['model', 'task', 'success', 'duration', 'total_tokens', 'cost', 'tokens_per_sec']\n",
    "display(df[display_cols].style.format({\n",
    "    'duration': '{:.2f}s',\n",
    "    'cost': '${:.6f}',\n",
    "    'tokens_per_sec': '{:.1f}',\n",
    "}))\n",
    "\n",
    "# Save raw results\n",
    "results_file = Path.cwd() / \"model_comparison_results.json\"\n",
    "with open(results_file, 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "print(f\"\\nüíæ Saved results to: {results_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí∞ Cost Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by model and calculate average cost\n",
    "cost_by_model = df.groupby('model')['cost'].agg(['mean', 'sum', 'count'])\n",
    "cost_by_model.columns = ['Avg Cost', 'Total Cost', 'Tasks']\n",
    "\n",
    "print(\"üí∞ Cost Summary by Model:\")\n",
    "display(cost_by_model.style.format({\n",
    "    'Avg Cost': '${:.6f}',\n",
    "    'Total Cost': '${:.6f}',\n",
    "}))\n",
    "\n",
    "# Visualize cost comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Average cost per task\n",
    "cost_by_model['Avg Cost'].plot(kind='bar', ax=ax1, color='steelblue')\n",
    "ax1.set_title('üí∞ Average Cost per Task', fontsize=14, fontweight='bold')\n",
    "ax1.set_ylabel('Cost ($)')\n",
    "ax1.set_xlabel('Model')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Total cost\n",
    "cost_by_model['Total Cost'].plot(kind='bar', ax=ax2, color='coral')\n",
    "ax2.set_title('üí∏ Total Cost (All Tasks)', fontsize=14, fontweight='bold')\n",
    "ax2.set_ylabel('Cost ($)')\n",
    "ax2.set_xlabel('Model')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('cost_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüì∏ Saved chart: cost_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö° Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance metrics\n",
    "perf_by_model = df.groupby('model').agg({\n",
    "    'duration': 'mean',\n",
    "    'tokens_per_sec': 'mean',\n",
    "    'success': 'mean',\n",
    "})\n",
    "perf_by_model.columns = ['Avg Duration (s)', 'Tokens/sec', 'Success Rate']\n",
    "\n",
    "print(\"‚ö° Performance Summary:\")\n",
    "display(perf_by_model.style.format({\n",
    "    'Avg Duration (s)': '{:.2f}',\n",
    "    'Tokens/sec': '{:.1f}',\n",
    "    'Success Rate': '{:.1%}',\n",
    "}))\n",
    "\n",
    "# Visualize performance\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Speed (tokens/sec)\n",
    "perf_by_model['Tokens/sec'].plot(kind='bar', ax=ax1, color='green')\n",
    "ax1.set_title('‚ö° Processing Speed', fontsize=14, fontweight='bold')\n",
    "ax1.set_ylabel('Tokens per Second')\n",
    "ax1.set_xlabel('Model')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Success rate\n",
    "(perf_by_model['Success Rate'] * 100).plot(kind='bar', ax=ax2, color='purple')\n",
    "ax2.set_title('‚úÖ Success Rate', fontsize=14, fontweight='bold')\n",
    "ax2.set_ylabel('Success Rate (%)')\n",
    "ax2.set_xlabel('Model')\n",
    "ax2.set_ylim([0, 105])\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('performance_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüì∏ Saved chart: performance_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Cost-Performance Analysis\n",
    "\n",
    "Find the best value: lowest cost with highest success rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine cost and performance\n",
    "combined = pd.DataFrame({\n",
    "    'Avg Cost': cost_by_model['Avg Cost'],\n",
    "    'Success Rate': perf_by_model['Success Rate'],\n",
    "    'Speed (tok/s)': perf_by_model['Tokens/sec'],\n",
    "    'Duration (s)': perf_by_model['Avg Duration (s)'],\n",
    "})\n",
    "\n",
    "# Calculate value score (higher is better)\n",
    "# Normalize: success rate is good, cost is bad\n",
    "combined['Value Score'] = (\n",
    "    combined['Success Rate'] * 100 / combined['Avg Cost'].replace(0, 0.000001)\n",
    ")\n",
    "\n",
    "combined = combined.sort_values('Value Score', ascending=False)\n",
    "\n",
    "print(\"üèÜ Cost-Performance Ranking:\")\n",
    "display(combined.style.format({\n",
    "    'Avg Cost': '${:.6f}',\n",
    "    'Success Rate': '{:.1%}',\n",
    "    'Speed (tok/s)': '{:.1f}',\n",
    "    'Duration (s)': '{:.2f}',\n",
    "    'Value Score': '{:.0f}',\n",
    "}).background_gradient(subset=['Value Score'], cmap='RdYlGn'))\n",
    "\n",
    "# Scatter plot: Cost vs Success Rate\n",
    "plt.figure(figsize=(10, 6))\n",
    "scatter = plt.scatter(\n",
    "    combined['Avg Cost'],\n",
    "    combined['Success Rate'] * 100,\n",
    "    s=combined['Speed (tok/s)'] * 2,  # Size = speed\n",
    "    alpha=0.6,\n",
    "    c=range(len(combined)),\n",
    "    cmap='viridis',\n",
    ")\n",
    "\n",
    "for idx, model in enumerate(combined.index):\n",
    "    plt.annotate(\n",
    "        model,\n",
    "        (combined.loc[model, 'Avg Cost'], combined.loc[model, 'Success Rate'] * 100),\n",
    "        xytext=(5, 5),\n",
    "        textcoords='offset points',\n",
    "        fontsize=9,\n",
    "        fontweight='bold',\n",
    "    )\n",
    "\n",
    "plt.xlabel('Average Cost ($)', fontsize=12)\n",
    "plt.ylabel('Success Rate (%)', fontsize=12)\n",
    "plt.title('üíé Cost vs Success Rate (bubble size = speed)', fontsize=14, fontweight='bold')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('cost_vs_performance.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüì∏ Saved chart: cost_vs_performance.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Summary & Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä EXPERIMENT SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "best_cost = combined['Avg Cost'].idxmin()\n",
    "best_success = combined['Success Rate'].idxmax()\n",
    "best_speed = combined['Speed (tok/s)'].idxmax()\n",
    "best_value = combined['Value Score'].idxmax()\n",
    "\n",
    "print(f\"\\nüèÜ Winners:\")\n",
    "print(f\"  üí∞ Cheapest:      {best_cost} (${combined.loc[best_cost, 'Avg Cost']:.6f}/task)\")\n",
    "print(f\"  ‚úÖ Most Reliable: {best_success} ({combined.loc[best_success, 'Success Rate']:.1%} success)\")\n",
    "print(f\"  ‚ö° Fastest:        {best_speed} ({combined.loc[best_speed, 'Speed (tok/s)']:.1f} tok/s)\")\n",
    "print(f\"  üíé Best Value:    {best_value} (score: {combined.loc[best_value, 'Value Score']:.0f})\")\n",
    "\n",
    "print(f\"\\nüí° Recommendations:\")\n",
    "if best_value == best_cost:\n",
    "    print(f\"  ‚Üí Use {best_value} for best cost-effectiveness\")\n",
    "else:\n",
    "    print(f\"  ‚Üí Use {best_value} for best overall value\")\n",
    "    print(f\"  ‚Üí Use {best_cost} if budget is critical\")\n",
    "    \n",
    "if best_speed != best_value:\n",
    "    print(f\"  ‚Üí Use {best_speed} when speed is important\")\n",
    "\n",
    "print(f\"\\nüìà Total Experiments: {len(results)}\")\n",
    "print(f\"üí∏ Total Cost: ${df['cost'].sum():.6f}\")\n",
    "print(f\"‚è±Ô∏è  Total Time: {df['duration'].sum():.2f}s\")\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
